#!/usr/bin/env python
# -*- coding: utf-8 -*-

# *************************************************************************** #
#                  Copyright Â© 2022, UChicago Argonne, LLC                    #
#                           All Rights Reserved                               #
#                         Software Name: Tomocupy                             #
#                     By: Argonne National Laboratory                         #
#                                                                             #
#                           OPEN SOURCE LICENSE                               #
#                                                                             #
# Redistribution and use in source and binary forms, with or without          #
# modification, are permitted provided that the following conditions are met: #
#                                                                             #
# 1. Redistributions of source code must retain the above copyright notice,   #
#    this list of conditions and the following disclaimer.                    #
# 2. Redistributions in binary form must reproduce the above copyright        #
#    notice, this list of conditions and the following disclaimer in the      #
#    documentation and/or other materials provided with the distribution.     #
# 3. Neither the name of the copyright holder nor the names of its            #
#    contributors may be used to endorse or promote products derived          #
#    from this software without specific prior written permission.            #
#                                                                             #
#                                                                             #
# *************************************************************************** #
#                               DISCLAIMER                                    #
#                                                                             #
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS         #
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT           #
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS           #
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT    #
# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,      #
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED    #
# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR      #
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF      #
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING        #
# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS          #
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.                #
# *************************************************************************** #

from tomocupy import utils
from tomocupy import logging
from tomocupy.reconstruction import fbp_filter
from tomocupy.reconstruction import lamfourierrec
from threading import Thread
import cupy as cp
import numpy as np
from tomocupy import logging

log = logging.getLogger(__name__)

class BackprojLamFourierParallel():

    def __init__(self, cl_conf, cl_writer):
        self.n0 = cl_conf.rh
        self.n1 = cl_conf.n
        self.n2 = cl_conf.n
        self.ntheta = cl_conf.nproj
        self.nthetac = cl_conf.ncproj
        self.detw = cl_conf.n
        self.deth = cl_conf.nz
        self.n1c = cl_conf.ncz
        self.dethc = cl_conf.ncz
        self.center = cl_conf.center
        self.ne = 4*self.detw

        
        self.cl_lamfourier = lamfourierrec.LamFourierRec(self.n0, self.n1, self.n2, self.ntheta, self.detw, self.deth, self.n1c, self.nthetac, self.dethc)        
        
        ################################
        s3 = [self.ntheta,self.deth,self.detw]
        s2 = [self.ntheta,self.deth,(self.detw//2+1)]
        s1 = [self.n1,(self.deth//2+1),self.n2]
        s0 = [self.n1,self.n0,self.n2]
        
        s5c = [self.nthetac,self.deth,self.detw]
        s4c = [self.nthetac,self.deth,self.detw//2+1]
        s3c = [2*self.ntheta,self.dethc,self.detw//2+1]        
        s2c = [self.n1,self.dethc,self.n2]
        s1c = [self.n1c,self.deth//2+1,self.n2]
        s0c = [self.n1c,self.n0,self.n2]
               
        global_block_size = max(np.prod(s0),np.prod(s1)*2,np.prod(s2)*2,np.prod(s3))
        gpu_block_size = max(np.prod(s0c),np.prod(s1c)*2,np.prod(s2c)*2,np.prod(s3c)*2,np.prod(s4c)*2,np.prod(s5c))
        
        self.pab0 = np.empty(global_block_size,dtype='float32')
        self.pab1 = np.empty(global_block_size,dtype='float32')
        
        self.pa33 =  self.pab0[:np.prod(s3)].reshape(s3)
        self.pa22 =  self.pab1[:np.prod(s2)*2].view('complex64').reshape(s2)        
        self.pa11 =  self.pab0[:np.prod(s1)*2].view('complex64').reshape(s1)        
        self.pa00 =  self.pab1[:np.prod(s0)].reshape(s0)
        
        
        self.gab0 = cp.empty(2*gpu_block_size,dtype='float32')
        self.gab1 = cp.empty(2*gpu_block_size,dtype='float32')
        self.gpab0 = utils.pinned_array(np.empty(gpu_block_size,dtype='float32'))
        self.gpab1 = utils.pinned_array(np.empty(gpu_block_size,dtype='float32'))
        
        self.ga55 = self.gab0[:2*np.prod(s5c)].reshape(2,*s5c)
        self.ga44 = self.gab1[:2*np.prod(s4c)*2].view('complex64').reshape(2,*s4c)
        self.ga33 = self.gab0[:2*np.prod(s3c)*2].view('complex64').reshape(2,*s3c)
        self.ga22 = self.gab1[:2*np.prod(s2c)*2].view('complex64').reshape(2,*s2c)
        self.ga11 = self.gab0[:2*np.prod(s1c)*2].view('complex64').reshape(2,*s1c)
        self.ga00 = self.gab1[:2*np.prod(s0c)].reshape(2,*s0c)        
        
        self.gpa55 = self.gpab0[:np.prod(s5c)].reshape(s5c)
        self.gpa44 = self.gpab1[:np.prod(s4c)*2].view('complex64').reshape(s4c)
        self.gpa33 = self.gpab0[:np.prod(s3c)*2].view('complex64').reshape(s3c)
        self.gpa22 = self.gpab1[:np.prod(s2c)*2].view('complex64').reshape(s2c)
        self.gpa11 = self.gpab0[:np.prod(s1c)*2].view('complex64').reshape(s1c)
        self.gpa00 = self.gpab1[:np.prod(s0c)].reshape(s0c)        
        ################################
        
        
        # streams for overlapping data transfers with computations
        self.stream1 = cp.cuda.Stream(non_blocking=False)
        self.stream2 = cp.cuda.Stream(non_blocking=False)
        self.stream3 = cp.cuda.Stream(non_blocking=False)

        # threads for data writing to disk
        self.write_threads = []
        for k in range(cl_conf.args.max_write_threads):
            self.write_threads.append(utils.WRThread())
        self.cl_filter = fbp_filter.FBPFilter(
                self.ne, self.deth, self.nthetac, cl_conf.args.dtype)  # note filter is applied on projections, not sinograms as in another methods
        self.wfilter = self.cl_filter.calc_filter(cl_conf.args.fbp_filter)
        self.cl_conf = cl_conf
        self.cl_writer = cl_writer
        self.rec_fun = self.rec_lam       
        
    def usfft1d_chunks(self, out_t, inp_t, out_gpu, inp_gpu, out_p, inp_p, phi):    
        log.info("usfft1d by chunks.")               
        nchunk = int(np.ceil(self.n1/self.n1c))
        
        for k in range(nchunk+2):
            utils.printProgressBar(
                k, nchunk+1, nchunk-k+1, length=40)
            if(k > 0 and k < nchunk+1):
                with self.stream2:# gpu computations
                    self.cl_lamfourier.usfft1d_adj(out_gpu[(k-1)%2], inp_gpu[(k-1)%2], phi, self.stream2)
            if(k > 1):
                with self.stream3: # gpu->cpu pinned copy
                    out_gpu[(k-2)%2].get(out=out_p)# contiguous copy, fast  # not swapaxes
                    
            if(k<nchunk):
                st, end = k*self.n1c, min(self.n1,(k+1)*self.n1c)
                s = end-st
                # inp_p[:s] = inp_t[st:end]
                utils.copy(inp_t[st:end],inp_p)
                with self.stream1:  
                    inp_gpu[k%2].set(inp_p)
                  
            self.stream3.synchronize()      
            
            if(k > 1):
                # cpu pinned->cpu copy    
                st, end = (k-2)*self.n1c, min(self.n1,(k-1)*self.n1c)
                s = end-st                    
                # out_t[st:end] = out_p[:s]
                utils.copy(out_p[:s],out_t[st:end])
                
            self.stream1.synchronize()
            self.stream2.synchronize()
                        
    def usfft2d_chunks(self, out, inp, out_gpu, inp_gpu, out_p, inp_p, theta, phi):
        log.info("usfft2d by chunks.")    
                
        theta = cp.array(theta)        
        nchunk = int(np.ceil((self.deth//2+1)/self.dethc))
        for k in range(nchunk+2):    
            utils.printProgressBar(
                k, nchunk+1, nchunk-k+1, length=40)                
            if(k > 0 and k < nchunk+1):
                with self.stream2: # gpu computations             
                    self.cl_lamfourier.usfft2d_adj(out_gpu[(k-1)%2], inp_gpu[(k-1)%2], theta, phi, k-1, self.stream2)
            if(k > 1):
                with self.stream3: # gpu->cpu copy
                    out_gpu[(k-2)%2].get(out=out_p)                    
                    
            if(k<nchunk):
                # cpu -> cpu pinned copy
                st, end = k*self.dethc, min(self.deth//2+1,(k+1)*self.dethc)                                        
                s = end-st            
                utils.copy(inp[:,st:end],inp_p[:self.ntheta,:s])
                # copy the flipped part of the array for handling r2c FFT
                if k==0:                    
                    utils.copy(inp[:,self.deth-end+1:self.deth-st+1],inp_p[self.ntheta:,-s:-1])
                    utils.copy(inp[:,0],inp_p[self.ntheta:,-1])
                else:
                    utils.copy(inp[:,self.deth-end+1:self.deth-st+1],inp_p[self.ntheta:,-s:])
                                                
                with self.stream1:  # cpu pinned->gpu copy                   
                    inp_gpu[k%2].set(inp_p)
                
            self.stream3.synchronize()                                                                        
            if (k > 1):
                # cpu pinned->cpu copy
                st, end = (k-2)*self.dethc, min(self.deth//2+1,(k-1)*self.dethc)
                s = end-st
                utils.copy(out_p[:,:s],out[:,st:end])                     
                        
            self.stream1.synchronize()
            self.stream2.synchronize()
                        
    def fft2_chunks(self, out, inp, out_gpu, inp_gpu, out_p, inp_p):
        log.info("fft2 by chunks.")
        
        nchunk = int(np.ceil(self.ntheta/self.nthetac))
        for k in range(nchunk+2):
            utils.printProgressBar(
                k, nchunk+1, nchunk-k+1, length=40)
            if(k > 0 and k < nchunk+1):
                with self.stream2: # gpu computations
                    data0 = inp_gpu[(k-1)%2]
                    data0 = self.fbp_filter_center(
                        data0, cp.tile(np.float32(0), [data0.shape[0], 1]))                 
                    self.cl_lamfourier.fft2d_fwd(out_gpu[(k-1)%2],data0,self.stream2)
            if(k > 1):
                with self.stream3:  # gpu->cpu pinned copy                            
                    out_gpu[(k-2)%2].get(out=out_p)
                                                                    
            if(k<nchunk):
                with self.stream1:  # cpu->gpu copy
                    st, end = k*self.nthetac, min(self.ntheta,(k+1)*self.nthetac)
                    s = end-st
                    utils.copy(inp[st:end],inp_p[:s])
                    inp_gpu[k%2].set(inp_p)

            self.stream3.synchronize()                                        
            if(k > 1):
                # cpu pinned ->cpu copy                
                st, end = (k-2)*self.nthetac, min(self.ntheta,(k-1)*self.nthetac)
                s = end-st
                utils.copy(out_p[:s],out[st:end])                
            self.stream1.synchronize()
            self.stream2.synchronize()
                 
    def fbp_filter_center(self, data, sht=0):
        """FBP filtering of projections with applying the rotation center shift wrt to the origin"""

        tmp = cp.pad(
            data, ((0, 0), (0, 0), (self.ne//2-self.n2//2, self.ne//2-self.n2//2)), mode='edge')
        t = cp.fft.rfftfreq(self.ne).astype('float32')
        w = self.wfilter*cp.exp(-2*cp.pi*1j*t*(-self.center +
                                    sht[:, cp.newaxis]+self.n2/2))  # center fix
        self.cl_filter.filter(tmp, w, cp.cuda.get_current_stream())
        data[:] = tmp[:, :, self.ne//2-self.n2//2:self.ne//2+self.n2//2]

        return data  # reuse input memory

    def rec_lam(self, data):        
        utils.copy(data,self.pa33)
        self.fft2_chunks(self.pa22, self.pa33, self.ga44, self.ga55,self.gpa44, self.gpa55)
        self.usfft2d_chunks(self.pa11, self.pa22, self.ga22, self.ga33, self.gpa22, self.gpa33, self.cl_conf.theta, np.pi/2+self.cl_conf.lamino_angle/180*np.pi)
        self.usfft1d_chunks(self.pa00,self.pa11,self.ga00,self.ga11,self.gpa00,self.gpa11, np.pi/2+self.cl_conf.lamino_angle/180*np.pi)         
        u = utils.copyTransposed(self.pa00)
        self.write_parallel(u)

    def write_parallel(self,u,nthreads=16):
        nchunk = int(np.ceil(u.shape[0]/nthreads))
        mthreads = []
        for k in range(nthreads):
            st = k*nchunk+self.cl_conf.lamino_start_row
            end = min((k+1)*nchunk,u.shape[0])+self.cl_conf.lamino_start_row
            th = Thread(target=self.cl_writer.write_data_chunk,args=(u[k*nchunk:min((k+1)*nchunk,u.shape[0])],st,end,k))
            mthreads.append(th)
            th.start()
        for th in mthreads:
            th.join()